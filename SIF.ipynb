{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smooth Inverse Frequency with MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Requeriments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras \n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Input, Dense, concatenate, Activation, Conv1D, GlobalMaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "import lib.xmlreader as xml\n",
    "import lib.utils as ut\n",
    "import numpy as np\n",
    "import random\n",
    "import gensim\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Google Drive Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "DATA_PATH='/content/drive/My Drive/TASS'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Process TASS Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_docs=xml.readXML(DATA_PATH+\"/2017/2017-train.xml\",[0,1,2,3])\n",
    "dev_docs=xml.readXML(DATA_PATH+\"/2017/2017-train.xml\",[0,1,2,3])\n",
    "test_docs=xml.readXML(DATA_PATH+\"/2017/2017-train.xml\",[0,1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = []\n",
    "for train_doc in train_docs:\n",
    "    train_labels.append(train_doc.polarity)\n",
    "dev_labels   = []\n",
    "for dev_doc in dev_docs:\n",
    "    dev_labels.append(dev_doc.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_train_docs = [train_docs[i] for i in range(len(train_labels)) if train_labels[i] == 0]\n",
    "NEG_train_docs = [train_docs[i] for i in range(len(train_labels)) if train_labels[i] == 1]\n",
    "NEU_train_docs = [train_docs[i] for i in range(len(train_labels)) if train_labels[i] == 2]\n",
    "NON_train_docs = [train_docs[i] for i in range(len(train_labels)) if train_labels[i] == 3]\n",
    "\n",
    "level_train_docs = [POSI_train_docs,NEGA_train_docs,NEUT_train_docs,NONE_train_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the same number of examples per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166\n"
     ]
    }
   ],
   "source": [
    "min_number = min(len(POSI_train_docs),len(NEGA_train_docs),len(NEUT_train_docs),len(NONE_train_docs))\n",
    "print(minSentLvl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_docs = []\n",
    "for i in range(len(level_train_docs)):\n",
    "    level_per = random.sample(level_train_docs[i],len(level_train_docs[i]))\n",
    "    new_train_docs.append(level_per[:min_number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shuf_train_docs size =  664\n"
     ]
    }
   ],
   "source": [
    "flat_train_docs = [item for sublist in new_train_docs for item in sublist]\n",
    "shuf_train_docs = random.sample(flat_train_docs,len(flat_train_docs))\n",
    "print(\"shuf_train_docs size = \", len(shuf_train_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for doc in shuf_train_docs + dev_docs + test_docs:\n",
    "    corpus.append(doc.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences =  2592\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentences = \", (len(test_docs + dev_docs + shuf_train_docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuf_train_labels = []\n",
    "for doc in shuf_train_docs:\n",
    "    shuf_train_labels.append(doc.polarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process with SBW 300 Features Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gensim_load_vec(path=\"../database/embeddings/SBW-vectors-300-min5.bin\"):\n",
    "    gensim_emb =  gensim.models.KeyedVectors.load_word2vec_format(path, binary=True)\n",
    "    vocab = gensim_emb.index2word\n",
    "    vec = gensim_emb.syn0\n",
    "    shape = gensim_emb.syn0.shape\n",
    "    return gensim_emb, vec, shape, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_emb, vec, shape, vocab = gensim_load_vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = CountVectorizer(tokenizer=ut.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2592, 8012)\n"
     ]
    }
   ],
   "source": [
    "X = counter.fit_transform(corpus)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2592, 8012)\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = X.shape[1]\n",
    "\n",
    "caption_texts = corpus\n",
    "Xc = counter.fit_transform(caption_texts).todense().astype(\"float\")\n",
    "print(Xc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2592, 1)\n"
     ]
    }
   ],
   "source": [
    "sent_lens = np.sum(Xc, axis=1).astype(\"float\")\n",
    "sent_lens[sent_lens == 0] = 1e-14\n",
    "print(sent_lens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((VOCAB_SIZE, 300), np.float)\n",
    "for word in list(counter.vocabulary_.keys()):\n",
    "    try:\n",
    "        i = counter.vocabulary_[word]\n",
    "        embedding_matrix[i] = gensim_emb[word]\n",
    "    except KeyError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2592, 300)\n"
     ]
    }
   ],
   "source": [
    "Xb = np.divide(np.dot(Xc, embedding_matrix), sent_lens)\n",
    "print(Xb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets = []\n",
    "train_labels = shuf_train_labels\n",
    "for doc in shuf_train_docs:\n",
    "    train_tweets.append(doc.content)\n",
    "\n",
    "dev_tweets = []\n",
    "for doc in dev_docs:\n",
    "    dev_tweets.append(doc.content)\n",
    "\n",
    "test_tweets = []\n",
    "for doc in test_docs:\n",
    "    test_tweets.append(doc.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(664, 300)\n",
      "(500, 300)\n",
      "(1428, 300)\n",
      "664\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "Xtrain = Xb[0:len(train_tweets)]\n",
    "Xdev   = Xb[ len(train_tweets):len(train_tweets) + len(dev_tweets)]\n",
    "Xtest  = Xb[-len(test_tweets):]\n",
    "ytrain = np.array(train_labels)\n",
    "ydev   = np.array(dev_labels)\n",
    "\n",
    "print(Xtrain.shape)\n",
    "print(Xdev.shape)\n",
    "print(Xtest.shape)\n",
    "\n",
    "print(len(train_labels))\n",
    "print(len(dev_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                19264     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4)                 260       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 19,524\n",
      "Trainable params: 19,524\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tweet_encoder = Input(shape=(300,), dtype='float32')\n",
    "join = Dense(300)(tweet_encoder)\n",
    "join = Dropout(0.5)\n",
    "join = Dense(128)(tweet_encoder)\n",
    "join = Dropout(0.5)\n",
    "join = Dense(64)(tweet_encoder)\n",
    "join = Dropout(0.2)(join)\n",
    "join = Dense(4)(join)\n",
    "output = Activation('softmax')(join)\n",
    "model  = Model(inputs=[tweet_encoder], outputs=[output])\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1164 samples, validate on 500 samples\n",
      "Epoch 1/50\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1.3856 - acc: 0.3352 - val_loss: 1.2411 - val_acc: 0.1380\n",
      "Epoch 2/50\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 1.3762 - acc: 0.3241 - val_loss: 1.2027 - val_acc: 0.1560\n",
      "Epoch 3/50\n",
      "1164/1164 [==============================] - 0s 25us/step - loss: 1.3704 - acc: 0.3215 - val_loss: 1.2057 - val_acc: 0.1540\n",
      "Epoch 4/50\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1.3696 - acc: 0.3137 - val_loss: 1.2071 - val_acc: 0.1520\n",
      "Epoch 5/50\n",
      "1164/1164 [==============================] - 0s 28us/step - loss: 1.3599 - acc: 0.3335 - val_loss: 1.1893 - val_acc: 0.1400\n",
      "Epoch 6/50\n",
      "1164/1164 [==============================] - 0s 28us/step - loss: 1.3645 - acc: 0.3318 - val_loss: 1.1410 - val_acc: 0.1020\n",
      "Epoch 7/50\n",
      "1164/1164 [==============================] - 0s 25us/step - loss: 1.3328 - acc: 0.3481 - val_loss: 1.0929 - val_acc: 0.1300\n",
      "Epoch 8/50\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 1.3316 - acc: 0.3430 - val_loss: 1.1587 - val_acc: 0.1560\n",
      "Epoch 9/50\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 1.3252 - acc: 0.3404 - val_loss: 1.1281 - val_acc: 0.1880\n",
      "Epoch 10/50\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1.3222 - acc: 0.3369 - val_loss: 1.1122 - val_acc: 0.1160\n",
      "Epoch 11/50\n",
      "1164/1164 [==============================] - 0s 25us/step - loss: 1.3164 - acc: 0.3490 - val_loss: 1.1269 - val_acc: 0.1760\n",
      "Epoch 12/50\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 1.3120 - acc: 0.3627 - val_loss: 1.0657 - val_acc: 0.1400\n",
      "Epoch 13/50\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 1.3103 - acc: 0.3533 - val_loss: 1.0810 - val_acc: 0.1260\n",
      "Epoch 14/50\n",
      "1164/1164 [==============================] - 0s 28us/step - loss: 1.3036 - acc: 0.3653 - val_loss: 1.0815 - val_acc: 0.1180\n",
      "Epoch 15/50\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 1.2967 - acc: 0.3713 - val_loss: 1.0616 - val_acc: 0.1460\n",
      "Epoch 16/50\n",
      "1164/1164 [==============================] - 0s 28us/step - loss: 1.2964 - acc: 0.3773 - val_loss: 1.0617 - val_acc: 0.1500\n",
      "Epoch 17/50\n",
      "1164/1164 [==============================] - 0s 27us/step - loss: 1.2936 - acc: 0.3662 - val_loss: 1.0994 - val_acc: 0.1140\n",
      "Epoch 18/50\n",
      "1164/1164 [==============================] - 0s 26us/step - loss: 1.2982 - acc: 0.3704 - val_loss: 1.0358 - val_acc: 0.1500\n",
      "Epoch 19/50\n",
      "1164/1164 [==============================] - 0s 26us/step - loss: 1.2941 - acc: 0.3601 - val_loss: 1.0305 - val_acc: 0.1660\n",
      "Epoch 20/50\n",
      "1164/1164 [==============================] - 0s 26us/step - loss: 1.2783 - acc: 0.3730 - val_loss: 1.0269 - val_acc: 0.1660\n",
      "Epoch 21/50\n",
      "1164/1164 [==============================] - 0s 29us/step - loss: 1.2854 - acc: 0.3756 - val_loss: 1.0766 - val_acc: 0.1220\n",
      "Epoch 22/50\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1.2859 - acc: 0.3765 - val_loss: 1.0144 - val_acc: 0.1680\n",
      "Epoch 23/50\n",
      "1164/1164 [==============================] - 0s 27us/step - loss: 1.2779 - acc: 0.3842 - val_loss: 1.0490 - val_acc: 0.2220\n",
      "Epoch 24/50\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 1.2691 - acc: 0.3885 - val_loss: 1.0198 - val_acc: 0.1720\n",
      "Epoch 25/50\n",
      "1164/1164 [==============================] - 0s 25us/step - loss: 1.2572 - acc: 0.3005 - val_loss: 1.0081 - val_acc: 0.2600\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(np.concatenate((Xtrain,Xdev)), to_categorical(np.concatenate((ytrain,ydev))),\n",
    "batch_size=128, epochs=25,validation_data=(Xdev, to_categorical(ydev)),verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
